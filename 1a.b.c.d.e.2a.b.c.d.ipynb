{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####For this problem you will experiment with linear regression models to make predictions with numerical data. You will also explore more systematic methods for feature selection and for optimizing model parameters (model selection). The data set you will use is a subset of the \"Communities and Crime\" data set that combines information from the 1990 census data as well as FBI crime data from 1995. Please read the full description of the data, including the description and statistics on different variables. The target attribute for regression purposes is \"ViolentCrimesPerPop\". The two identifier attributes \"state\" and \"community name\" should be excluded for the regression task.\n",
    "\n",
    "####Your tasks in this problem are the following [Note: for these tasks you will use the available linear-models from scikit-learn as well as the implementations of the relevant approaches from the Ch. 8 of MLA] .\n",
    "\n",
    "####1a) Load and preprocess the data using Pandas or Numpy and, if necessary, preprocessing functions from scikit-learn. The provided data is already normalized (see description), so there is no need for additional normalization. Compute and display basic statistics (mean, standard deviation, min, max, etc.) for each of the variables in the data set. Separate the target attribute for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read the dataset and make sure to use na_values parameters to track down all the na and missing values.\n",
    "ccData=pd.read_csv('./communities/communities.csv', na_values=[\"?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>communityname</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>...</th>\n",
       "      <th>NumStreet</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state        communityname  population  householdsize  racepctblack  \\\n",
       "0      8         Lakewoodcity        0.19           0.33          0.02   \n",
       "1     53          Tukwilacity        0.00           0.16          0.12   \n",
       "2     24         Aberdeentown        0.00           0.42          0.49   \n",
       "3     34  Willingborotownship        0.04           0.77          1.00   \n",
       "4     42    Bethlehemtownship        0.01           0.55          0.02   \n",
       "\n",
       "   racePctWhite  racePctAsian  racePctHisp  agePct12t21  agePct12t29  \\\n",
       "0          0.90          0.12         0.17         0.34         0.47   \n",
       "1          0.74          0.45         0.07         0.26         0.59   \n",
       "2          0.56          0.17         0.04         0.39         0.47   \n",
       "3          0.08          0.12         0.10         0.51         0.50   \n",
       "4          0.95          0.09         0.05         0.38         0.38   \n",
       "\n",
       "          ...           NumStreet  PctForeignBorn  PctBornSameState  \\\n",
       "0         ...                 0.0            0.12              0.42   \n",
       "1         ...                 0.0            0.21              0.50   \n",
       "2         ...                 0.0            0.14              0.49   \n",
       "3         ...                 0.0            0.19              0.30   \n",
       "4         ...                 0.0            0.11              0.72   \n",
       "\n",
       "   PctSameHouse85  PctSameCity85  PctSameState85  LandArea  PopDens  \\\n",
       "0            0.50           0.51            0.64      0.12     0.26   \n",
       "1            0.34           0.60            0.52      0.02     0.12   \n",
       "2            0.54           0.67            0.56      0.01     0.21   \n",
       "3            0.73           0.64            0.65      0.02     0.39   \n",
       "4            0.64           0.61            0.53      0.04     0.09   \n",
       "\n",
       "   PctUsePubTrans  ViolentCrimesPerPop  \n",
       "0            0.20                 0.20  \n",
       "1            0.45                 0.67  \n",
       "2            0.02                 0.43  \n",
       "3            0.28                 0.12  \n",
       "4            0.02                 0.03  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OtherPerCap    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check any missing or na values\n",
    "missing_tab=ccData.isnull().sum(0)\n",
    "missing_tab[missing_tab>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, there is one missing value from on the attribute 'OtherPerCap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fill in the missing value in the 'OtherPerCap' attribute by its MEAN value \n",
    "ccData.OtherPerCap.fillna(ccData.OtherPerCap.mean(axis=0), axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Double check if there is any more missing values\n",
    "missing_tab=ccData.isnull().sum(0)\n",
    "sum(missing_tab[missing_tab>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>28.683551</td>\n",
       "      <td>16.397553</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.057593</td>\n",
       "      <td>0.126906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>householdsize</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.463395</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racepctblack</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.179629</td>\n",
       "      <td>0.253442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctWhite</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.753716</td>\n",
       "      <td>0.244039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctAsian</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.153681</td>\n",
       "      <td>0.208877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctHisp</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.144022</td>\n",
       "      <td>0.232492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct12t21</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.424218</td>\n",
       "      <td>0.155196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct12t29</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.493867</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct16t24</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.336264</td>\n",
       "      <td>0.166505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct65up</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.423164</td>\n",
       "      <td>0.179185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.5300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numbUrban</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.064072</td>\n",
       "      <td>0.128256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctUrban</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.696269</td>\n",
       "      <td>0.444811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medIncome</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.361123</td>\n",
       "      <td>0.209362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWWage</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.558154</td>\n",
       "      <td>0.182913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWFarmSelf</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.291570</td>\n",
       "      <td>0.204108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWInvInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.495687</td>\n",
       "      <td>0.178071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.6200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWSocSec</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.471133</td>\n",
       "      <td>0.173619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWPubAsst</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.317778</td>\n",
       "      <td>0.222137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWRetire</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>0.167564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medFamInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.375677</td>\n",
       "      <td>0.198257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perCapInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.350251</td>\n",
       "      <td>0.191109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitePerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.368049</td>\n",
       "      <td>0.186804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.291098</td>\n",
       "      <td>0.171593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1725</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indianPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.203506</td>\n",
       "      <td>0.164775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AsianPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.322357</td>\n",
       "      <td>0.195411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OtherPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.284742</td>\n",
       "      <td>0.190960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HispPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.386279</td>\n",
       "      <td>0.183081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumUnderPov</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.055507</td>\n",
       "      <td>0.127941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctPopUnderPov</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.303024</td>\n",
       "      <td>0.228474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedNumBR</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.314694</td>\n",
       "      <td>0.255182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HousVacant</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.076815</td>\n",
       "      <td>0.150465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctHousOccup</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.719549</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctHousOwnOcc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.548686</td>\n",
       "      <td>0.185204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctVacantBoarded</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.204529</td>\n",
       "      <td>0.217770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctVacMore6Mos</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.433335</td>\n",
       "      <td>0.188986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedYrHousBuilt</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.494178</td>\n",
       "      <td>0.232467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctHousNoPhone</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.264478</td>\n",
       "      <td>0.242847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctWOFullPlumb</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.243059</td>\n",
       "      <td>0.206295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnOccLowQuart</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.264689</td>\n",
       "      <td>0.224425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnOccMedVal</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.263490</td>\n",
       "      <td>0.231542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnOccHiQuart</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.268942</td>\n",
       "      <td>0.235252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RentLowQ</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.346379</td>\n",
       "      <td>0.219323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RentMedian</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.372457</td>\n",
       "      <td>0.209278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RentHighQ</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.422964</td>\n",
       "      <td>0.248286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedRent</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.384102</td>\n",
       "      <td>0.213404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.5300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedRentPctHousInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.490125</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedOwnCostPctInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.449754</td>\n",
       "      <td>0.187274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedOwnCostPctIncNoMtg</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.403816</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumInShelters</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>0.102607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumStreet</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.022778</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.215552</td>\n",
       "      <td>0.231134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctBornSameState</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.608892</td>\n",
       "      <td>0.204329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.535050</td>\n",
       "      <td>0.181352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctSameCity85</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.626424</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctSameState85</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.651530</td>\n",
       "      <td>0.198221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LandArea</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>0.109459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PopDens</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.232854</td>\n",
       "      <td>0.203092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.161685</td>\n",
       "      <td>0.229055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.237979</td>\n",
       "      <td>0.232985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count       mean        std  min      25%     50%  \\\n",
       "state                  1994.0  28.683551  16.397553  1.0  12.0000  34.000   \n",
       "population             1994.0   0.057593   0.126906  0.0   0.0100   0.020   \n",
       "householdsize          1994.0   0.463395   0.163717  0.0   0.3500   0.440   \n",
       "racepctblack           1994.0   0.179629   0.253442  0.0   0.0200   0.060   \n",
       "racePctWhite           1994.0   0.753716   0.244039  0.0   0.6300   0.850   \n",
       "racePctAsian           1994.0   0.153681   0.208877  0.0   0.0400   0.070   \n",
       "racePctHisp            1994.0   0.144022   0.232492  0.0   0.0100   0.040   \n",
       "agePct12t21            1994.0   0.424218   0.155196  0.0   0.3400   0.400   \n",
       "agePct12t29            1994.0   0.493867   0.143564  0.0   0.4100   0.480   \n",
       "agePct16t24            1994.0   0.336264   0.166505  0.0   0.2500   0.290   \n",
       "agePct65up             1994.0   0.423164   0.179185  0.0   0.3000   0.420   \n",
       "numbUrban              1994.0   0.064072   0.128256  0.0   0.0000   0.030   \n",
       "pctUrban               1994.0   0.696269   0.444811  0.0   0.0000   1.000   \n",
       "medIncome              1994.0   0.361123   0.209362  0.0   0.2000   0.320   \n",
       "pctWWage               1994.0   0.558154   0.182913  0.0   0.4400   0.560   \n",
       "pctWFarmSelf           1994.0   0.291570   0.204108  0.0   0.1600   0.230   \n",
       "pctWInvInc             1994.0   0.495687   0.178071  0.0   0.3700   0.480   \n",
       "pctWSocSec             1994.0   0.471133   0.173619  0.0   0.3500   0.475   \n",
       "pctWPubAsst            1994.0   0.317778   0.222137  0.0   0.1425   0.260   \n",
       "pctWRetire             1994.0   0.479248   0.167564  0.0   0.3600   0.470   \n",
       "medFamInc              1994.0   0.375677   0.198257  0.0   0.2300   0.330   \n",
       "perCapInc              1994.0   0.350251   0.191109  0.0   0.2200   0.300   \n",
       "whitePerCap            1994.0   0.368049   0.186804  0.0   0.2400   0.320   \n",
       "blackPerCap            1994.0   0.291098   0.171593  0.0   0.1725   0.250   \n",
       "indianPerCap           1994.0   0.203506   0.164775  0.0   0.1100   0.170   \n",
       "AsianPerCap            1994.0   0.322357   0.195411  0.0   0.1900   0.280   \n",
       "OtherPerCap            1994.0   0.284742   0.190960  0.0   0.1700   0.250   \n",
       "HispPerCap             1994.0   0.386279   0.183081  0.0   0.2600   0.345   \n",
       "NumUnderPov            1994.0   0.055507   0.127941  0.0   0.0100   0.020   \n",
       "PctPopUnderPov         1994.0   0.303024   0.228474  0.0   0.1100   0.250   \n",
       "...                       ...        ...        ...  ...      ...     ...   \n",
       "MedNumBR               1994.0   0.314694   0.255182  0.0   0.0000   0.500   \n",
       "HousVacant             1994.0   0.076815   0.150465  0.0   0.0100   0.030   \n",
       "PctHousOccup           1994.0   0.719549   0.194024  0.0   0.6300   0.770   \n",
       "PctHousOwnOcc          1994.0   0.548686   0.185204  0.0   0.4300   0.540   \n",
       "PctVacantBoarded       1994.0   0.204529   0.217770  0.0   0.0600   0.130   \n",
       "PctVacMore6Mos         1994.0   0.433335   0.188986  0.0   0.2900   0.420   \n",
       "MedYrHousBuilt         1994.0   0.494178   0.232467  0.0   0.3500   0.520   \n",
       "PctHousNoPhone         1994.0   0.264478   0.242847  0.0   0.0600   0.185   \n",
       "PctWOFullPlumb         1994.0   0.243059   0.206295  0.0   0.1000   0.190   \n",
       "OwnOccLowQuart         1994.0   0.264689   0.224425  0.0   0.0900   0.180   \n",
       "OwnOccMedVal           1994.0   0.263490   0.231542  0.0   0.0900   0.170   \n",
       "OwnOccHiQuart          1994.0   0.268942   0.235252  0.0   0.0900   0.180   \n",
       "RentLowQ               1994.0   0.346379   0.219323  0.0   0.1700   0.310   \n",
       "RentMedian             1994.0   0.372457   0.209278  0.0   0.2000   0.330   \n",
       "RentHighQ              1994.0   0.422964   0.248286  0.0   0.2200   0.370   \n",
       "MedRent                1994.0   0.384102   0.213404  0.0   0.2100   0.340   \n",
       "MedRentPctHousInc      1994.0   0.490125   0.169500  0.0   0.3700   0.480   \n",
       "MedOwnCostPctInc       1994.0   0.449754   0.187274  0.0   0.3200   0.450   \n",
       "MedOwnCostPctIncNoMtg  1994.0   0.403816   0.192593  0.0   0.2500   0.370   \n",
       "NumInShelters          1994.0   0.029438   0.102607  0.0   0.0000   0.000   \n",
       "NumStreet              1994.0   0.022778   0.100400  0.0   0.0000   0.000   \n",
       "PctForeignBorn         1994.0   0.215552   0.231134  0.0   0.0600   0.130   \n",
       "PctBornSameState       1994.0   0.608892   0.204329  0.0   0.4700   0.630   \n",
       "PctSameHouse85         1994.0   0.535050   0.181352  0.0   0.4200   0.540   \n",
       "PctSameCity85          1994.0   0.626424   0.200521  0.0   0.5200   0.670   \n",
       "PctSameState85         1994.0   0.651530   0.198221  0.0   0.5600   0.700   \n",
       "LandArea               1994.0   0.065231   0.109459  0.0   0.0200   0.040   \n",
       "PopDens                1994.0   0.232854   0.203092  0.0   0.1000   0.170   \n",
       "PctUsePubTrans         1994.0   0.161685   0.229055  0.0   0.0200   0.070   \n",
       "ViolentCrimesPerPop    1994.0   0.237979   0.232985  0.0   0.0700   0.150   \n",
       "\n",
       "                           75%   max  \n",
       "state                  42.0000  56.0  \n",
       "population              0.0500   1.0  \n",
       "householdsize           0.5400   1.0  \n",
       "racepctblack            0.2300   1.0  \n",
       "racePctWhite            0.9400   1.0  \n",
       "racePctAsian            0.1700   1.0  \n",
       "racePctHisp             0.1600   1.0  \n",
       "agePct12t21             0.4700   1.0  \n",
       "agePct12t29             0.5400   1.0  \n",
       "agePct16t24             0.3600   1.0  \n",
       "agePct65up              0.5300   1.0  \n",
       "numbUrban               0.0700   1.0  \n",
       "pctUrban                1.0000   1.0  \n",
       "medIncome               0.4900   1.0  \n",
       "pctWWage                0.6900   1.0  \n",
       "pctWFarmSelf            0.3700   1.0  \n",
       "pctWInvInc              0.6200   1.0  \n",
       "pctWSocSec              0.5800   1.0  \n",
       "pctWPubAsst             0.4400   1.0  \n",
       "pctWRetire              0.5800   1.0  \n",
       "medFamInc               0.4800   1.0  \n",
       "perCapInc               0.4300   1.0  \n",
       "whitePerCap             0.4400   1.0  \n",
       "blackPerCap             0.3800   1.0  \n",
       "indianPerCap            0.2500   1.0  \n",
       "AsianPerCap             0.4000   1.0  \n",
       "OtherPerCap             0.3600   1.0  \n",
       "HispPerCap              0.4800   1.0  \n",
       "NumUnderPov             0.0500   1.0  \n",
       "PctPopUnderPov          0.4500   1.0  \n",
       "...                        ...   ...  \n",
       "MedNumBR                0.5000   1.0  \n",
       "HousVacant              0.0700   1.0  \n",
       "PctHousOccup            0.8600   1.0  \n",
       "PctHousOwnOcc           0.6700   1.0  \n",
       "PctVacantBoarded        0.2700   1.0  \n",
       "PctVacMore6Mos          0.5600   1.0  \n",
       "MedYrHousBuilt          0.6700   1.0  \n",
       "PctHousNoPhone          0.4200   1.0  \n",
       "PctWOFullPlumb          0.3300   1.0  \n",
       "OwnOccLowQuart          0.4000   1.0  \n",
       "OwnOccMedVal            0.3900   1.0  \n",
       "OwnOccHiQuart           0.3800   1.0  \n",
       "RentLowQ                0.4900   1.0  \n",
       "RentMedian              0.5200   1.0  \n",
       "RentHighQ               0.5900   1.0  \n",
       "MedRent                 0.5300   1.0  \n",
       "MedRentPctHousInc       0.5900   1.0  \n",
       "MedOwnCostPctInc        0.5800   1.0  \n",
       "MedOwnCostPctIncNoMtg   0.5100   1.0  \n",
       "NumInShelters           0.0100   1.0  \n",
       "NumStreet               0.0000   1.0  \n",
       "PctForeignBorn          0.2800   1.0  \n",
       "PctBornSameState        0.7775   1.0  \n",
       "PctSameHouse85          0.6600   1.0  \n",
       "PctSameCity85           0.7700   1.0  \n",
       "PctSameState85          0.7900   1.0  \n",
       "LandArea                0.0700   1.0  \n",
       "PopDens                 0.2800   1.0  \n",
       "PctUsePubTrans          0.1900   1.0  \n",
       "ViolentCrimesPerPop     0.3300   1.0  \n",
       "\n",
       "[99 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we check on the statistics of every attribute\n",
    "ccData.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Split the original table into predictor set and target variable\n",
    "x=ccData.ix[:,0:-1] #Predictor set\n",
    "y=ccData.ViolentCrimesPerPop # Target/Response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Drop the identifier attribute 'state'\n",
    "x.drop('state', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Drop the identifier attribute 'communityname'\n",
    "x.drop('communityname', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.057593</td>\n",
       "      <td>0.126906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>householdsize</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.463395</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racepctblack</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.179629</td>\n",
       "      <td>0.253442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctWhite</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.753716</td>\n",
       "      <td>0.244039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctAsian</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.153681</td>\n",
       "      <td>0.208877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctHisp</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.144022</td>\n",
       "      <td>0.232492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct12t21</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.424218</td>\n",
       "      <td>0.155196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct12t29</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.493867</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct16t24</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.336264</td>\n",
       "      <td>0.166505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agePct65up</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.423164</td>\n",
       "      <td>0.179185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.5300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numbUrban</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.064072</td>\n",
       "      <td>0.128256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctUrban</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.696269</td>\n",
       "      <td>0.444811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medIncome</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.361123</td>\n",
       "      <td>0.209362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWWage</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.558154</td>\n",
       "      <td>0.182913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWFarmSelf</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.291570</td>\n",
       "      <td>0.204108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWInvInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.495687</td>\n",
       "      <td>0.178071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.6200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWSocSec</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.471133</td>\n",
       "      <td>0.173619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWPubAsst</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.317778</td>\n",
       "      <td>0.222137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWRetire</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>0.167564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medFamInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.375677</td>\n",
       "      <td>0.198257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perCapInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.350251</td>\n",
       "      <td>0.191109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitePerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.368049</td>\n",
       "      <td>0.186804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.291098</td>\n",
       "      <td>0.171593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1725</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indianPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.203506</td>\n",
       "      <td>0.164775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AsianPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.322357</td>\n",
       "      <td>0.195411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OtherPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.284742</td>\n",
       "      <td>0.190960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HispPerCap</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.386279</td>\n",
       "      <td>0.183081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumUnderPov</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.055507</td>\n",
       "      <td>0.127941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctPopUnderPov</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.303024</td>\n",
       "      <td>0.228474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctLess9thGrade</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.315807</td>\n",
       "      <td>0.213360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctHousLess3BR</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.495186</td>\n",
       "      <td>0.172508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedNumBR</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.314694</td>\n",
       "      <td>0.255182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HousVacant</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.076815</td>\n",
       "      <td>0.150465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctHousOccup</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.719549</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctHousOwnOcc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.548686</td>\n",
       "      <td>0.185204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctVacantBoarded</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.204529</td>\n",
       "      <td>0.217770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctVacMore6Mos</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.433335</td>\n",
       "      <td>0.188986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedYrHousBuilt</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.494178</td>\n",
       "      <td>0.232467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctHousNoPhone</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.264478</td>\n",
       "      <td>0.242847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctWOFullPlumb</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.243059</td>\n",
       "      <td>0.206295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnOccLowQuart</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.264689</td>\n",
       "      <td>0.224425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnOccMedVal</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.263490</td>\n",
       "      <td>0.231542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnOccHiQuart</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.268942</td>\n",
       "      <td>0.235252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RentLowQ</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.346379</td>\n",
       "      <td>0.219323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RentMedian</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.372457</td>\n",
       "      <td>0.209278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RentHighQ</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.422964</td>\n",
       "      <td>0.248286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedRent</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.384102</td>\n",
       "      <td>0.213404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.5300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedRentPctHousInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.490125</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedOwnCostPctInc</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.449754</td>\n",
       "      <td>0.187274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedOwnCostPctIncNoMtg</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.403816</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumInShelters</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>0.102607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumStreet</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.022778</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.215552</td>\n",
       "      <td>0.231134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctBornSameState</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.608892</td>\n",
       "      <td>0.204329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.535050</td>\n",
       "      <td>0.181352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctSameCity85</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.626424</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctSameState85</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.651530</td>\n",
       "      <td>0.198221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LandArea</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>0.109459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PopDens</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.232854</td>\n",
       "      <td>0.203092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.161685</td>\n",
       "      <td>0.229055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count      mean       std  min     25%    50%     75%  \\\n",
       "population             1994.0  0.057593  0.126906  0.0  0.0100  0.020  0.0500   \n",
       "householdsize          1994.0  0.463395  0.163717  0.0  0.3500  0.440  0.5400   \n",
       "racepctblack           1994.0  0.179629  0.253442  0.0  0.0200  0.060  0.2300   \n",
       "racePctWhite           1994.0  0.753716  0.244039  0.0  0.6300  0.850  0.9400   \n",
       "racePctAsian           1994.0  0.153681  0.208877  0.0  0.0400  0.070  0.1700   \n",
       "racePctHisp            1994.0  0.144022  0.232492  0.0  0.0100  0.040  0.1600   \n",
       "agePct12t21            1994.0  0.424218  0.155196  0.0  0.3400  0.400  0.4700   \n",
       "agePct12t29            1994.0  0.493867  0.143564  0.0  0.4100  0.480  0.5400   \n",
       "agePct16t24            1994.0  0.336264  0.166505  0.0  0.2500  0.290  0.3600   \n",
       "agePct65up             1994.0  0.423164  0.179185  0.0  0.3000  0.420  0.5300   \n",
       "numbUrban              1994.0  0.064072  0.128256  0.0  0.0000  0.030  0.0700   \n",
       "pctUrban               1994.0  0.696269  0.444811  0.0  0.0000  1.000  1.0000   \n",
       "medIncome              1994.0  0.361123  0.209362  0.0  0.2000  0.320  0.4900   \n",
       "pctWWage               1994.0  0.558154  0.182913  0.0  0.4400  0.560  0.6900   \n",
       "pctWFarmSelf           1994.0  0.291570  0.204108  0.0  0.1600  0.230  0.3700   \n",
       "pctWInvInc             1994.0  0.495687  0.178071  0.0  0.3700  0.480  0.6200   \n",
       "pctWSocSec             1994.0  0.471133  0.173619  0.0  0.3500  0.475  0.5800   \n",
       "pctWPubAsst            1994.0  0.317778  0.222137  0.0  0.1425  0.260  0.4400   \n",
       "pctWRetire             1994.0  0.479248  0.167564  0.0  0.3600  0.470  0.5800   \n",
       "medFamInc              1994.0  0.375677  0.198257  0.0  0.2300  0.330  0.4800   \n",
       "perCapInc              1994.0  0.350251  0.191109  0.0  0.2200  0.300  0.4300   \n",
       "whitePerCap            1994.0  0.368049  0.186804  0.0  0.2400  0.320  0.4400   \n",
       "blackPerCap            1994.0  0.291098  0.171593  0.0  0.1725  0.250  0.3800   \n",
       "indianPerCap           1994.0  0.203506  0.164775  0.0  0.1100  0.170  0.2500   \n",
       "AsianPerCap            1994.0  0.322357  0.195411  0.0  0.1900  0.280  0.4000   \n",
       "OtherPerCap            1994.0  0.284742  0.190960  0.0  0.1700  0.250  0.3600   \n",
       "HispPerCap             1994.0  0.386279  0.183081  0.0  0.2600  0.345  0.4800   \n",
       "NumUnderPov            1994.0  0.055507  0.127941  0.0  0.0100  0.020  0.0500   \n",
       "PctPopUnderPov         1994.0  0.303024  0.228474  0.0  0.1100  0.250  0.4500   \n",
       "PctLess9thGrade        1994.0  0.315807  0.213360  0.0  0.1600  0.270  0.4200   \n",
       "...                       ...       ...       ...  ...     ...    ...     ...   \n",
       "PctHousLess3BR         1994.0  0.495186  0.172508  0.0  0.4000  0.510  0.6000   \n",
       "MedNumBR               1994.0  0.314694  0.255182  0.0  0.0000  0.500  0.5000   \n",
       "HousVacant             1994.0  0.076815  0.150465  0.0  0.0100  0.030  0.0700   \n",
       "PctHousOccup           1994.0  0.719549  0.194024  0.0  0.6300  0.770  0.8600   \n",
       "PctHousOwnOcc          1994.0  0.548686  0.185204  0.0  0.4300  0.540  0.6700   \n",
       "PctVacantBoarded       1994.0  0.204529  0.217770  0.0  0.0600  0.130  0.2700   \n",
       "PctVacMore6Mos         1994.0  0.433335  0.188986  0.0  0.2900  0.420  0.5600   \n",
       "MedYrHousBuilt         1994.0  0.494178  0.232467  0.0  0.3500  0.520  0.6700   \n",
       "PctHousNoPhone         1994.0  0.264478  0.242847  0.0  0.0600  0.185  0.4200   \n",
       "PctWOFullPlumb         1994.0  0.243059  0.206295  0.0  0.1000  0.190  0.3300   \n",
       "OwnOccLowQuart         1994.0  0.264689  0.224425  0.0  0.0900  0.180  0.4000   \n",
       "OwnOccMedVal           1994.0  0.263490  0.231542  0.0  0.0900  0.170  0.3900   \n",
       "OwnOccHiQuart          1994.0  0.268942  0.235252  0.0  0.0900  0.180  0.3800   \n",
       "RentLowQ               1994.0  0.346379  0.219323  0.0  0.1700  0.310  0.4900   \n",
       "RentMedian             1994.0  0.372457  0.209278  0.0  0.2000  0.330  0.5200   \n",
       "RentHighQ              1994.0  0.422964  0.248286  0.0  0.2200  0.370  0.5900   \n",
       "MedRent                1994.0  0.384102  0.213404  0.0  0.2100  0.340  0.5300   \n",
       "MedRentPctHousInc      1994.0  0.490125  0.169500  0.0  0.3700  0.480  0.5900   \n",
       "MedOwnCostPctInc       1994.0  0.449754  0.187274  0.0  0.3200  0.450  0.5800   \n",
       "MedOwnCostPctIncNoMtg  1994.0  0.403816  0.192593  0.0  0.2500  0.370  0.5100   \n",
       "NumInShelters          1994.0  0.029438  0.102607  0.0  0.0000  0.000  0.0100   \n",
       "NumStreet              1994.0  0.022778  0.100400  0.0  0.0000  0.000  0.0000   \n",
       "PctForeignBorn         1994.0  0.215552  0.231134  0.0  0.0600  0.130  0.2800   \n",
       "PctBornSameState       1994.0  0.608892  0.204329  0.0  0.4700  0.630  0.7775   \n",
       "PctSameHouse85         1994.0  0.535050  0.181352  0.0  0.4200  0.540  0.6600   \n",
       "PctSameCity85          1994.0  0.626424  0.200521  0.0  0.5200  0.670  0.7700   \n",
       "PctSameState85         1994.0  0.651530  0.198221  0.0  0.5600  0.700  0.7900   \n",
       "LandArea               1994.0  0.065231  0.109459  0.0  0.0200  0.040  0.0700   \n",
       "PopDens                1994.0  0.232854  0.203092  0.0  0.1000  0.170  0.2800   \n",
       "PctUsePubTrans         1994.0  0.161685  0.229055  0.0  0.0200  0.070  0.1900   \n",
       "\n",
       "                       max  \n",
       "population             1.0  \n",
       "householdsize          1.0  \n",
       "racepctblack           1.0  \n",
       "racePctWhite           1.0  \n",
       "racePctAsian           1.0  \n",
       "racePctHisp            1.0  \n",
       "agePct12t21            1.0  \n",
       "agePct12t29            1.0  \n",
       "agePct16t24            1.0  \n",
       "agePct65up             1.0  \n",
       "numbUrban              1.0  \n",
       "pctUrban               1.0  \n",
       "medIncome              1.0  \n",
       "pctWWage               1.0  \n",
       "pctWFarmSelf           1.0  \n",
       "pctWInvInc             1.0  \n",
       "pctWSocSec             1.0  \n",
       "pctWPubAsst            1.0  \n",
       "pctWRetire             1.0  \n",
       "medFamInc              1.0  \n",
       "perCapInc              1.0  \n",
       "whitePerCap            1.0  \n",
       "blackPerCap            1.0  \n",
       "indianPerCap           1.0  \n",
       "AsianPerCap            1.0  \n",
       "OtherPerCap            1.0  \n",
       "HispPerCap             1.0  \n",
       "NumUnderPov            1.0  \n",
       "PctPopUnderPov         1.0  \n",
       "PctLess9thGrade        1.0  \n",
       "...                    ...  \n",
       "PctHousLess3BR         1.0  \n",
       "MedNumBR               1.0  \n",
       "HousVacant             1.0  \n",
       "PctHousOccup           1.0  \n",
       "PctHousOwnOcc          1.0  \n",
       "PctVacantBoarded       1.0  \n",
       "PctVacMore6Mos         1.0  \n",
       "MedYrHousBuilt         1.0  \n",
       "PctHousNoPhone         1.0  \n",
       "PctWOFullPlumb         1.0  \n",
       "OwnOccLowQuart         1.0  \n",
       "OwnOccMedVal           1.0  \n",
       "OwnOccHiQuart          1.0  \n",
       "RentLowQ               1.0  \n",
       "RentMedian             1.0  \n",
       "RentHighQ              1.0  \n",
       "MedRent                1.0  \n",
       "MedRentPctHousInc      1.0  \n",
       "MedOwnCostPctInc       1.0  \n",
       "MedOwnCostPctIncNoMtg  1.0  \n",
       "NumInShelters          1.0  \n",
       "NumStreet              1.0  \n",
       "PctForeignBorn         1.0  \n",
       "PctBornSameState       1.0  \n",
       "PctSameHouse85         1.0  \n",
       "PctSameCity85          1.0  \n",
       "PctSameState85         1.0  \n",
       "LandArea               1.0  \n",
       "PopDens                1.0  \n",
       "PctUsePubTrans         1.0  \n",
       "\n",
       "[97 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check on predictor variables statistics\n",
    "x.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1994.000000\n",
       "mean        0.237979\n",
       "std         0.232985\n",
       "min         0.000000\n",
       "25%         0.070000\n",
       "50%         0.150000\n",
       "75%         0.330000\n",
       "max         1.000000\n",
       "Name: ViolentCrimesPerPop, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check on the target variable statistic\n",
    "y.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1b) Perform standard linear regression on data using the implementation for Ch. 8 of MLA. Compute the RMSE value on the full training data. Also, plot the correlation between the predicted and actual values of the target attribute. Display the obtained regression coefficients (weights). Finally, perform 10-fold cross-validation and compare the cross-validation RMSE to the training RMSE (for cross validation, you may use the KFold module from sklearn.cross_validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1=np.array(x)\n",
    "y1=np.array(y)\n",
    "# Should to add a column of ONEs for x0 in order to do multiple regression\n",
    "x1=np.array([np.concatenate((v,[1])) for v in x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLA Functions from textbook Machine Learning in Action, Chapter 8:\n",
    "def standRegres(xArr,yArr):\n",
    "    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n",
    "    xTx = xMat.T*xMat\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print (\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T*yMat)\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.31108068e-01],\n",
       "        [ -3.14114977e-02],\n",
       "        [  2.09909670e-01],\n",
       "        [ -4.05351612e-02],\n",
       "        [ -1.38892919e-02],\n",
       "        [  5.89726825e-02],\n",
       "        [  1.23399025e-01],\n",
       "        [ -2.22621600e-01],\n",
       "        [ -1.47500199e-01],\n",
       "        [  5.01635477e-02],\n",
       "        [ -2.42413829e-01],\n",
       "        [  4.64024392e-02],\n",
       "        [ -1.96945615e-01],\n",
       "        [ -2.06117500e-01],\n",
       "        [  4.65935490e-02],\n",
       "        [ -1.77212915e-01],\n",
       "        [  6.30148504e-02],\n",
       "        [  1.14942190e-02],\n",
       "        [ -9.08951848e-02],\n",
       "        [  2.74640044e-01],\n",
       "        [  1.01752476e-01],\n",
       "        [ -3.31517562e-01],\n",
       "        [ -2.91799268e-02],\n",
       "        [ -3.54483393e-02],\n",
       "        [  2.26173855e-02],\n",
       "        [  4.30950137e-02],\n",
       "        [  3.44408548e-02],\n",
       "        [  1.28412458e-01],\n",
       "        [ -1.91293360e-01],\n",
       "        [ -1.00769002e-01],\n",
       "        [  6.46856092e-02],\n",
       "        [  1.06062117e-01],\n",
       "        [  2.44126016e-06],\n",
       "        [  2.34984611e-01],\n",
       "        [ -3.75705330e-02],\n",
       "        [ -7.74957660e-03],\n",
       "        [  4.66779619e-01],\n",
       "        [  2.26295907e-01],\n",
       "        [  1.74621953e-01],\n",
       "        [ -5.75206227e-01],\n",
       "        [ -1.41954207e-01],\n",
       "        [  5.68782538e-02],\n",
       "        [ -3.51066745e-01],\n",
       "        [ -3.49493414e-02],\n",
       "        [  4.63705978e-04],\n",
       "        [  5.57016681e-02],\n",
       "        [ -1.82238360e-01],\n",
       "        [ -1.54646442e-01],\n",
       "        [  1.26172899e-01],\n",
       "        [ -1.44320569e-01],\n",
       "        [  2.39071713e-02],\n",
       "        [  3.33390229e-02],\n",
       "        [ -7.42297409e-02],\n",
       "        [  3.59876412e-02],\n",
       "        [ -3.31691535e-02],\n",
       "        [ -2.18174916e-01],\n",
       "        [  4.45777391e-01],\n",
       "        [ -2.00030978e-01],\n",
       "        [ -2.67307658e-02],\n",
       "        [ -1.41457254e-01],\n",
       "        [  6.38133109e-02],\n",
       "        [ -2.10115806e-01],\n",
       "        [  6.51276465e-01],\n",
       "        [ -8.02774919e-02],\n",
       "        [ -2.53817057e-01],\n",
       "        [ -6.66334925e-01],\n",
       "        [  2.01002575e-01],\n",
       "        [  1.03326247e-01],\n",
       "        [  2.88599766e-02],\n",
       "        [  1.68314795e-01],\n",
       "        [ -4.00752791e-02],\n",
       "        [  5.53867355e-01],\n",
       "        [  4.70396419e-02],\n",
       "        [ -7.64314747e-02],\n",
       "        [ -2.89277350e-02],\n",
       "        [  1.40739015e-02],\n",
       "        [ -1.40629951e-02],\n",
       "        [ -3.46854609e-01],\n",
       "        [  2.67796471e-01],\n",
       "        [  1.19446906e-02],\n",
       "        [ -2.36996317e-01],\n",
       "        [ -2.60764386e-02],\n",
       "        [ -6.84041742e-02],\n",
       "        [  3.74730887e-01],\n",
       "        [  4.17402525e-02],\n",
       "        [ -4.45747318e-02],\n",
       "        [ -8.34683479e-02],\n",
       "        [  1.30736305e-01],\n",
       "        [  1.83468559e-01],\n",
       "        [  1.26046949e-01],\n",
       "        [  4.63490658e-03],\n",
       "        [ -2.24577196e-02],\n",
       "        [  2.88627319e-02],\n",
       "        [  1.30622513e-02],\n",
       "        [  2.76170980e-02],\n",
       "        [ -1.24479622e-02],\n",
       "        [ -3.73099899e-02],\n",
       "        [  5.88079813e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_standRegres=standRegres(x1,y1)\n",
    "w_standRegres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=np.mat(x1)\n",
    "predict_y=np.dot(x1,w_standRegres) #Predict the dependent variable based on the above calculated weights \n",
    "predict_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_y=np.array(predict_y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y2=predict_y[0]\n",
    "predict_y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can constuct a vector of errors\n",
    "err = abs(predict_y2-y1.T)\n",
    "err=np.mat(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse_train=np.sqrt(np.dot(err,err.T) / len(predict_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Create linear regression object\n",
    "linreg = LinearRegression()\n",
    "n = 10\n",
    "kf = KFold(len(x1), n_folds=n)\n",
    "xval_err = 0\n",
    "for train,test in kf:\n",
    "    linreg.fit(x1[train],y1[train])\n",
    "    p = linreg.predict(x1[test])\n",
    "    e = p-y1[test]\n",
    "    xval_err += np.sqrt(np.dot(e,e)/len(x1[test]))\n",
    "       \n",
    "rmse_10cv = xval_err/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training RMSE is 0.12889\n",
      "The 10fold RMSE is 0.13586\n"
     ]
    }
   ],
   "source": [
    "print('The Training RMSE is %0.5f'%(rmse_train))\n",
    "print('The 10fold RMSE is %0.5f'%(rmse_10cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, the RMSE between Training and 10-fold are very close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Actual Target Value from original Data\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicted value from the\n",
    "predict_y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUfU4DdDYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110572748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the Predicted Value vs Actual Value\n",
    "plt.plot(y1,predict_y2,'ro', color='blue')\n",
    "plt.plot([0,1],[0,1],'red')\n",
    "plt.xlabel('Actual Target')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted Value vs Actual Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.83294957],\n",
       "       [ 0.83294957,  1.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correlation between predicted value and Actual Value\n",
    "corrcoef(predict_y2, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1c) Feature Selection:  use the scikit-learn regression model from sklearn.linear_model with a subset of features to perform linear regression. For feature selection, write a script or function that takes as input the training data, target attribute; the model; and any other parameters you find necessary, and returns the optimal percentage of the most informative features to use. Your approach should use k-fold cross-validation on the training data (you can use k=5). You can use feature_selection.SelectPercentile to find the most informative variables. Show the list of most informative variables and their weights [Note: since this is regression not classification, you should use feature_selection.f_regression as scoring function rather than chi2). Next, plot the model's mean absolute error values  on cross-validation relative to the percentage of selected features (See scikit-learn's metrics.mean_absolute_error). In order to use cross_validation.cross_val_score with regression you'll need to pass to it scoring='mean_absolute_error' as a parameter. [Hint: for an example of a similar feature selection process please review the class example notebook. Also, review scikit-learn documentation for feature selection.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x1=np.array(x1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1.T, test_size=0.2, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595, 98)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399, 98)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.113943060315\n",
      "6 0.101191612456\n",
      "11 0.101175759705\n",
      "16 0.101243083971\n",
      "21 0.0995874757738\n",
      "26 0.0982839780838\n",
      "31 0.0976837268074\n",
      "36 0.0971086224375\n",
      "41 0.0966454393648\n",
      "46 0.0974334227416\n",
      "51 0.097762665564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 0.0979647775442\n",
      "61 0.0987830572765\n",
      "66 0.099202338531\n",
      "71 0.0995516982625\n",
      "76 0.0984921834617\n",
      "81 0.098671167702\n",
      "86 0.0985200638453\n",
      "91 0.0990166587181\n",
      "96 0.098333824917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Create linear regression object\n",
    "linreg = LinearRegression()\n",
    "results=[]\n",
    "n_fold=5 #Set number of fold =5\n",
    "percentiles = range(1, 100, 5) #Pre-set the the precentile\n",
    "for i in range(1,100,n_fold):\n",
    "    fs = feature_selection.SelectPercentile(feature_selection.f_regression ,percentile=i)\n",
    "    x_train_fs = fs.fit_transform(x_train,y_train)\n",
    "    scores = abs(cross_validation.cross_val_score(linreg,x_train_fs,y_train,cv=5,scoring='mean_absolute_error'))\n",
    "    print(i,scores.mean())\n",
    "    results = np.append(results, scores.mean())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal percentile of features: 41\n",
      "Optimal number of features: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "percentiles = range(1, 100, 5)\n",
    "optimal_percentile = np.where(results == results.min())[0]\n",
    "print('Optimal percentile of features:',percentiles[optimal_percentile])\n",
    "optimal_num_features = (int(percentiles[optimal_percentile])/100) * len(x.columns)\n",
    "print('Optimal number of features: %2d' %(optimal_num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEKCAYAAAAmfuNnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncEREJCmUcEREJCmUcEREJCn+P5kJ2e+FJ4JRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1105a7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(percentiles,results)\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Cross Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "fs = feature_selection.SelectPercentile(feature_selection.f_regression, percentile=41)\n",
    "x_train_fs = fs.fit_transform(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population \t 208.41261328\n",
      "racepctblack \t 1047.17072994\n",
      "racePctWhite \t 1378.54255124\n",
      "numbUrban \t 203.280067113\n",
      "medIncome \t 340.039081698\n",
      "pctWInvInc \t 759.277958878\n",
      "pctWPubAsst \t 775.367280937\n",
      "medFamInc \t 369.968950665\n",
      "perCapInc \t 214.476272283\n",
      "NumUnderPov \t 351.350284421\n",
      "PctPopUnderPov \t 601.321078238\n",
      "PctLess9thGrade \t 350.660916764\n",
      "PctNotHSGrad \t 501.428584848\n",
      "PctBSorMore \t 170.828982428\n",
      "PctUnemployed \t 535.048270241\n",
      "PctEmploy \t 202.929512551\n",
      "MalePctDivorce \t 600.416653191\n",
      "FemalePctDiv \t 694.198504598\n",
      "TotalPctDiv \t 686.288270843\n",
      "PctFam2Par \t 1577.7293989\n",
      "PctKids2Par \t 1878.58699592\n",
      "PctYoungKids2Par \t 1255.59323019\n",
      "PctTeen2Par \t 1236.42780275\n",
      "NumIlleg \t 426.473129708\n",
      "PctIlleg \t 1873.74692879\n",
      "PctNotSpeakEnglWell \t 174.364894226\n",
      "PctLargHouseFam \t 285.663993303\n",
      "PctPersOwnOccup \t 609.905473623\n",
      "PctPersDenseHous \t 426.422975995\n",
      "PctHousLess3BR \t 462.104835912\n",
      "MedNumBR \t 229.82615215\n",
      "HousVacant \t 315.227840688\n",
      "PctHousOccup \t 178.762852021\n",
      "PctHousOwnOcc \t 446.41763617\n",
      "PctVacantBoarded \t 456.44099955\n",
      "PctHousNoPhone \t 505.577838129\n",
      "PctWOFullPlumb \t 237.818953272\n",
      "MedRentPctHousInc \t 182.459184417\n",
      "NumInShelters \t 226.998104942\n",
      "NumStreet \t 172.049431351\n"
     ]
    }
   ],
   "source": [
    "#The following shows the optimal features and their scores\n",
    "for i in range(len(x.columns.values)):\n",
    "    if fs.get_support()[i]:\n",
    "        print (x.columns.values[i], '\\t', fs.scores_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####1d) Next, perform Ridge Regression and Lasso Regression, however this time use the modules from sklearn.linear_model. In each case, perform systematic model selection to identify the optimal alpha parameter. First, create a 20%-80% randomized split of the data. Set aside the test portion; the model selection process should be performed using the 80% training data partition. You should create a function that takes as input the data and target attribute; the parameter to vary and a list of its values; the model to be trained; and any other relevant input needed to determine the optimal value for the specified parameter. The model selection process should perform k-fold cross validation (k should be a parameter, but you can select k=5 for this problem). You should also plot the error values on the training and cross-validation splits across the specified values of the alpha parameter. Finally, using the best alpha value, run the model on the set-aside test data. Discuss your observation and conclusions. [Hint: for an example of a similar model selection process please review the class example notebook.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Assign Ridge Regression and Lasso Regression\n",
    "ridreg = Ridge()\n",
    "lasreg = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#Split the train (80%) and test(20%) dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1.T, test_size=0.2, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}\n",
      "{'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': False, 'positive': False, 'precompute': False, 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#Check Ridge Regression parameters\n",
    "print (ridreg.get_params())\n",
    "print (lasreg.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_params(x, y, clf, param_values, param_name, K):\n",
    "    train_scores = np.zeros(len(param_values))\n",
    "    test_scores = np.zeros(len(param_values))\n",
    "    \n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "\n",
    "\n",
    "    for i , param_value in enumerate(param_values):\n",
    "        clf.set_params(**{param_name:param_value})\n",
    "        cv = KFold(len(x), K, shuffle=True, random_state=0)\n",
    "\n",
    "        mae_err_train=np.zeros(K)\n",
    "        mae_err_test=np.zeros(K)\n",
    "\n",
    "\n",
    "        for j , (train, test) in enumerate(cv):\n",
    "            clf.fit([x[k] for k in train],y[train])\n",
    "            train_predict=clf.predict(x[train])\n",
    "            test_predict=clf.predict(x[test])\n",
    "            #Find the Mean Absolute Error for the Regression on Train and Test result\n",
    "            mae_err_train[j]=mean_absolute_error(y[train],train_predict)\n",
    "            mae_err_test[j]=mean_absolute_error(y[test],test_predict)\n",
    "        \n",
    "        #Take the average of the Mean Absolute Error for every parameter value\n",
    "        train_scores[i]=np.mean(mae_err_train)\n",
    "        test_scores[i]=np.mean(mae_err_test)\n",
    "    \n",
    "    #Find the Best score for test result\n",
    "    min_test_scores = test_scores[np.where(test_scores==test_scores.min())][0]\n",
    "    #min_test_scores = min(test_scores)\n",
    "    #Find the best parameter value\n",
    "    best_param = param_values[np.where(test_scores==test_scores.min())][0]\n",
    "    #best_param = param_values[argmin(test_scores)]\n",
    "    \n",
    "    \n",
    "    plt.plot(param_values,train_scores, color='blue', label='Train',alpha=0.4)\n",
    "    plt.plot(param_values,test_scores, color='red', label='Test',alpha=0.4)\n",
    "    plt.xlabel('Alpha Values')\n",
    "    plt.ylabel('K Fold Cross Validation MAE Error Accuracy')\n",
    "    plt.legend(loc=7)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_scores , test_scores, min_test_scores, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110499a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum MAE Value is 0.09511\n",
      "The best parameter alpha is 4.00170\n"
     ]
    }
   ],
   "source": [
    "#Create parameter : alpha value list\n",
    "alpha= np.linspace(0.0001,500,2500)\n",
    "\n",
    "#Set up the function entries\n",
    "x = x_train #80% of the original dataset\n",
    "y = y_train\n",
    "clf = ridreg #Ridge Regression model\n",
    "param_values = alpha  #Parameter: alpha values\n",
    "param_name = 'alpha' #Parameter name\n",
    "K = 5 #Number of KFold\n",
    "\n",
    "#Call the calc_params function for Ridge Regression\n",
    "a,b,c,d=calc_params(x, y, clf, param_values, param_name,K)\n",
    "print('The minimum MAE Value is %0.5f'% (c))\n",
    "print('The best parameter alpha is %0.5f'%(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Ridge Regression on Test Dataset is  0.0960277361877\n",
      "RMSE of Ridge Regression on Test Dataset is  0.131205099539\n"
     ]
    }
   ],
   "source": [
    "#Use the Best parameter: alpha run for the 20% Test Set\n",
    "ridreg.set_params(alpha=4.00170) #Use the best parameter: alpha for predicting the test set\n",
    "ridreg.fit(x_train,y_train)\n",
    "test_predict = ridreg.predict(x_test)\n",
    "MAE=mean_absolute_error(y_test,test_predict) #Mean Absolute Error of test set\n",
    "err= test_predict - y_test\n",
    "rmse=np.sqrt(np.dot(err,err)/len(y_test))\n",
    "print('MAE of Ridge Regression on Test Dataset is ', MAE)\n",
    "print('RMSE of Ridge Regression on Test Dataset is ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm8VXW9//HXm0lQQIJIDVDQaz/CUsQTjg2WY5Z6zTHt\nGmpkj7xOaZd+dX9Oda/+Su815UaUKJbpz9Iecc0h9apZmgyKE4giTgdREAeoEDnw+f2x1sHt8ey1\n1zmctfc++7yfj8d+nDXvz3LJ+Zz1HRURmJmZVdKr1gGYmVn34IRhZma5OGGYmVkuThhmZpaLE4aZ\nmeXihGFmZrk4YZiZWS5OGGZmlkuhCUPSQZIWSVosaUo7+8dKelDSWknntNl3lqQnJT0h6XpJ/YuM\n1czMsqmont6SegNPA/sDzcAc4LiIWFByzIeA7YDDgTci4kfp9hHAn4BxEbFG0o3ArRFxTdZ3fvCD\nH4zRo0cXcDdmZo1p3rx5r0XE8DzH9ikwjonA4ohYAiDpBuAwYGPCiIjlwHJJh5SJbYCkdcDmwMuV\nvnD06NHMnTu3K2I3M+sRJL2Q99iKRVLpm0JnjABeKllvTrdVFBFLgR8BLwLLgLci4g+djMPMzLpA\nnjqMZyT9UNK4wqNJSfoAydvIGODDwBaSTihz7GRJcyXNXbFiRbVCNDPrcfIkjF1I6iJ+Lukv6S/o\nwTnOWwqMKlkfmW7LYz/guYhYERHrgJuBvdo7MCKmR0RTRDQNH56rGM7MzDqhYsKIiNUR8bOI2Av4\nF+A8YJmkmZL+IePUOcCOksZI6gccC8zKGdeLwB6SNpck4HPAwpznmplZASpWeqd1GIcAk4DRwKXA\ndcAngVuBj7R3XkS0SDoNuAPoDcyIiCclnZrunyZpa2AuMBjYIOlMkpZRD0n6DfAw0AI8AkzflBs1\nM7NNU7FZraQlwD3AVRHxQJt9P46I0wuMr0OamprCraTMzPKTNC8imvIcm6dZ7c4R8df2dtRTsjAz\ns2LlSRhTJZ0REW/CxhZMl0bEScWGVj1rmlfy+rznah2GmVmn9NqsL9sctEvh35P3DePN1pWIeEPS\nrgXGVHVP//pR3pq3uNZhmJl1Sq+Bm9dNwugl6QMR8QaApKE5z+s21rcEvYYOYdz5R9c6FDOzDutV\npWFk8/zivxR4UNKvAQFHAj8oNKpqi6BXbxg6tNaBmJnVr4oJIyKulTQP2DfddETpAIJmZtYz5Cpa\nSvtPrAD6A0jaNiJeLDQyMzOrK3kGHzxU0jPAc8B9wPPAbQXHZWZmdSZPVclFwB7A0xExhmSYjr8U\nGpWZmdWdPAljXUSsJGkt1Ssi7gFy9Qo0M7PGkacO401JA4E/AtdJWg78rdiwzMys3uR5wzgM+Dtw\nFnA78CzwxSKDqroIkkFxzcysnMw3jHSk2lsiYl9gAzCzKlFVWUHTmpuZNZTMN4yIWE8y7PiWVYrH\nzMzqVJ46jL8Cj0u6k5K6C49Ua2bWs+RJGDenn4YWrsMwM8uUZ2iQhqy3eI8InC7MzLLlmaL1OeB9\n1cIRsX0hEZmZWV3KUyRV2kmvP3AU4HFdzcx6mIr9MCJiZclnaUT8J3BIFWIzM7M6kqdIakLJai+S\nN46GmkDJzMwqyzuBUqsWklFrPTWdmVkPk6eV1L6VjjEzs8aXZz6Mf5M0pGT9A5K+X2xYVRYB7odh\nZpYpz+CDB0fEm60rEfEG8PniQqoR5wszs0x5EkZvSZu1rkgaAGyWcXy348EHzcwqy1PpfR1wt6Sr\n0/VJNOiotWZmVl6eSu9LJD0K7Jduuigi7ig2rFpwmZSZWZY8/TDGAPdGxO3p+gBJoyPi+aKDqxa9\nf+QTMzNrI08dxq9JJk9qtT7d1lDcSMrMLFuehNEnIt5pXUmX+xUXUvW50tvMrLI8CWOFpENbVyQd\nBrxWXEhmZlaP8rSSOhW4TtKVJDXDLwH/VGhUZmZWd/K0knoW2EPSwHT9r5K2KjwyMzOrK3mKpFr1\nAY6RdDfwSEHx1I5rvc3MMmW+YaS9ug8DvgzsCgwCDgf+WHxoVeRabzOzisq+YUj6FfA0sD9wBTAa\neCMi7o2IDeXOMzOzxpRVJDUOeANYCCyMiPW0M7d3FkkHSVokabGkKe3sHyvpQUlrJZ3TZt8QSb+R\n9JSkhZL27Mh3m5lZ1ypbJBUR4yWNBY4D7pL0GjBI0lYR8WqlC0vqDUwleUNpBuZImhURC0oOex04\nnaSYq63Lgdsj4khJ/YDNc99VB7lEysysssxK74h4KiLOi4ixwBkkgw7OkfRAjmtPBBZHxJK0s98N\nJPUhpddfHhFzgHWl2yVtCXwKuCo97p3SIdYL4UpvM7NMuVtJRcS8iDgH2A54X/FSO0aQ9Nlo1Zxu\ny2MMsAK4WtIjkn4uaYu8sXaUx5IyM6usI81qAYhE0a2k+gATgJ9ExK7A3yiTpCRNljRX0twVK1YU\nHJaZWc/V4YTRAUuBUSXrI9NteTQDzRHxULr+G5IE8j4RMT0imiKiafjw4Z0O1iVSZmbZMhOGpF6S\nju7ktecAO0oak1ZaHwvMynNiRLwCvCTpf6WbPgcsyDhlk7jS28ysssyOexGxQdK3gRs7euGIaJF0\nGnAH0BuYERFPSjo13T9N0tbAXGAwsEHSmcC4iFgF/DPJGFb9gCUkM/2ZmVmN5Bl88K60j8T/I6lL\nACAiXq90YkTcCtzaZtu0kuVXSIqq2jt3PtCUIz4zM6uCPAnjmPTnN0u2BbB914dTO+EpWs3MMuUZ\nrXZMNQKpKVdimJlVlGdO777AN0g60gHcC/w0ItaVPcnMzBpOniKpnwB9gf9K17+SbjulqKBqwc1q\nzcyy5UkYn4iIXUrW/0fSo0UFZGZm9SlPx731knZoXZG0PbC+uJBqxK8YZmaZ8rxhnAvcI2kJyZze\n29FofSJc6W1mVlGlGfd6AWuAHYHWXteLImJt0YGZmVl9ydPTe2o6AOBjVYrJzMzqUJ46jLslfUlq\n3EJ+l0iZmVWWJ2F8Hfg1sFbSKkmrJa0qOC4zM6szleowBOwUES9WKR4zM6tTlaZoDeD3VYqlthq3\nxM3MrEvkKZJ6WNInCo+kplyJYWZWSZ5+GLsDx0t6gWR4c5G8fOxcaGRmZlZX8iSMAwuPwszM6l7Z\nIilJnwWIiBeAXhHxQusH2K1aAVaLqzDMzLJl1WH8qGT5pjb7vldALLUTEM4YZmaZshKGyiy3t969\nueeemVlFWQkjyiy3t25mZg0uq9J7e0mzSN4mWpdJ1xt/2lYzM3uPrIRxWMnyj9rsa7ve7TVWGZuZ\nWdcrmzAi4r5qBlJLEU4YZmaV5OnpbWZm5oSxkZvVmpll6lTCkJSnh3i3ITf6MjOrKKun959Kln/R\nZvfswiIyM7O6lPWGsUXJ8k5t9rn8xsysh8nbca8j+8zMrAFl1UUMkfSPJElliKQj0u0Ctiw8smpz\npbeZWaashHEfcGjJ8hdL9v2xsIhqIDaEy9jMzCrI6rg3qdw+SVsVE46ZmdWr3M1qJQ2RdLKku4FH\nCozJzMzqUGZ/CkkDSMaU+jKwKzAIOJwGK5IyM7PKsvph/Ap4GtgfuAIYDbwREfdGxIbqhFc9rvM2\nM8uWVSQ1DngDWAgsjIj1uDmtmVmPVTZhRMR44GiSYqi70p7fgxq2wtuvGGZmmTIrvSPiqYg4LyLG\nAmcA1wJzJD2Q5+KSDpK0SNJiSVPa2T9W0oOS1ko6p539vSU9IumWnPfTOZ6i1cysotyDCEbEPGBe\n+ov9k5WOl9QbmEpSB9JMkmhmRcSCksNeB04nqUhvzxkkRWKD88ZpZmbFKJswJP24wrmVWkpNBBZH\nxJL0ejeQtLjamDAiYjmwXNIh7Xz/SOAQ4AfA2RW+y8ysQ9atW0dzczNvv/12rUOpiv79+zNy5Ej6\n9u3b6WtkvWGcCjwB3Ai8TMcHHBwBvFSy3gzs3oHz/xP4NkkdSlmSJgOTAbbddtsOhmhmPVVzczOD\nBg1i9OjRqMHrMCOClStX0tzczJgxYzp9naw6jG2A6cCBwFeAvsDvImJmRMzs9DfmIOkLwPK0GCxT\nREyPiKaIaBo+fPimfGnnzzWzbuftt99m2LBhDZ8sACQxbNiwTX6bymoltTIipkXEvsAkYAiwQNJX\ncl57KTCqZH1kui2PvYFDJT0P3AB8VtIvc57bca70NuuRekKyaNUV91pxaBBJE0gqn08AbgMq/tWf\nmgPsKGmMpH7AscCsPCdGxHciYmREjE7P+5+IOCHn95qZ1b2VK1cyfvx4xo8fz9Zbb82IESM2rr/z\nzju5rjFp0iQWLVpUcKTvyqr0vpCk0nkhyV/534mIlrwXjogWSacBdwC9gRkR8aSkU9P90yRtDcwl\naQW1QdKZwLiIWNXpOzIz6waGDRvG/PnzATj//PMZOHAg55zz3t4FEUFE0KtX+3/bX3311YXHWSrr\nDeN7JMVQuwD/Djws6TFJj0t6LM/FI+LWiPhIROwQET9It02LiGnp8ivpm8TgiBiSLq9qc417I+IL\nnbo7M7NuZvHixYwbN47jjz+enXbaiWXLljF58mSamprYaaeduPDCCzceu88++zB//nxaWloYMmQI\nU6ZMYZdddmHPPfdk+fLlXR5bViupzleld0M9qCjTzNp44AFYubJrrzlsGOy1V+fOfeqpp7j22mtp\namoC4OKLL2bo0KG0tLSw7777cuSRRzJu3Lj3nPPWW2/x6U9/mosvvpizzz6bGTNmMGXK+/pLb5Ks\n+TBe6NJvqmOu8zazerLDDjtsTBYA119/PVdddRUtLS28/PLLLFiw4H0JY8CAARx88MEA7Lbbbtx/\n//1dHlfunt4Nz68YZj1WZ98EirLFFltsXH7mmWe4/PLLmT17NkOGDOGEE05ot3lsv379Ni737t2b\nlpbcVc655Z5AqaH5FcPM6tSqVasYNGgQgwcPZtmyZdxxxx01i8VvGGZmdWzChAmMGzeOsWPHst12\n27H33nvXLBZFhb+uJe0NnA9sR5JgBEREbF94dB3U1NQUc+fO7fB5fzn3JvoMGUjTdw8sICozq0cL\nFy7kox/9aK3DqKr27lnSvIhoKnPKe+R5w7gKOIukw976DkdoZmYNIU/CeCsibis8khqLDo+taGbW\ns+RJGPdI+iFwM7C2dWNEPFxYVFUXbiRlZlZBnoTROiR5aRlXAJ/t+nBqxI2kzMwqqpgw0tFqzcys\nh8szWu2Wki6TNDf9XCppy2oEZ2Zm9SNPx70ZwGrg6PSzCqjuEIlmZg2mK4Y3B5gxYwavvPJKgZG+\nK08dxg4R8aWS9QskzS8qoFpxpbeZVVOe4c3zmDFjBhMmTGDrrbfu6hDfJ0/CWCNpn4j4E2zsyLem\n2LCqK8LNas2sfsycOZOpU6fyzjvvsNdee3HllVeyYcMGJk2axPz584kIJk+ezFZbbcX8+fM55phj\nGDBgALNnz37PmFJdLU/C+AYwM623EPA68NXCIqoJN5My69HqaHzzJ554gt/+9rc88MAD9OnTh8mT\nJ3PDDTewww478Nprr/H4448D8OabbzJkyBCuuOIKrrzySsaPH9+18bcjTyup+cAukgan654Nz8ys\nIHfddRdz5szZOLz5mjVrGDVqFAceeCCLFi3i9NNP55BDDuGAAw6oemxZU7SeEBG/lHR2m+0ARMRl\nBcdmZlYddTS+eURw0kkncdFFF71v32OPPcZtt93G1KlTuemmm5g+fXpVY8tqJdU6IPugdj4DC47L\nzKxH2m+//bjxxht57bXXgKQ11YsvvsiKFSuICI466iguvPBCHn44GWxj0KBBrF69uiqxZc2499N0\n8a6I+HPpvrTiu7G4mZSZ1YGPf/zjnHfeeey3335s2LCBvn37Mm3aNHr37s3JJ59MRCCJSy65BIBJ\nkyZxyimnVKXSO8/w5g9HxIRK2+pBp4c3P/tG+nxoKE1T9isgKjOrRx7ePNElw5tL2hPYCxjeph5j\nMNC7E7HWNb9gmJlly2ol1Y+krqIPSb1Fq1XAkUUGVW2eodXMrLKsOoz7gPskXRMRL1QxJjMzq0N5\nOu79PZ0PYyegf+vGiGic4c3NrEdqrUDuCSrVV+eRZ/DB64CngDHABcDzwJxN/mYzsxrq378/K1eu\n7JJfpPUuIli5ciX9+/evfHCGPG8YwyLiKklnlBRTNV7C6CF/ZZhZYuTIkTQ3N7NixYpah1IV/fv3\nZ+TIkZt0jTwJY136c5mkQ4CXgaGb9K31pgf8hWFm79W3b1/GjBlT6zC6lTwJ4/vpwIPfAq4gaVZ7\nVqFRmZlZ3ckz+OAt6eJbgKdrNTProbI67l1BxrjfEXF6IRGZmVldymolNReYR9KUdgLwTPoZT9Kp\nr7G40tvMLFNWx72ZAJK+AewTES3p+jTg/uqEVyWu9DYzqyhPP4wPkFR0txqYbmsofsEwM8uWp5XU\nxcAjku4hmaL1U8D5RQZVbX6/MDOrLE8rqasl3Qbsnm76l4h4pdiwzMys3pQtkpI0Nv05Afgw8FL6\n+XC6rSJJB0laJGmxpCntfYekByWtlXROyfZRku6RtEDSk5LO6OiNmZlZ18p6w/gW8DXg0nb2BZA5\n+KCk3sBUYH+gGZgjaVZELCg57HXgdODwNqe3AN+KiIclDQLmSbqzzblmZlZFWa2kvpb+7GxnvYnA\n4ohYAiDpBuAwYOMv/YhYDixPhxwp/e5lwLJ0ebWkhcCI0nO7nGu9zcwyZXXcOyLrxIi4ucK1R5AU\nYbVq5t16kNwkjQZ2BR7q6Lm5v8PNas3MKsoqkvpixr4AKiWMTSZpIHATcGZErCpzzGRgMsC2227b\nqe9xvjAzqyyrSGrSJl57KTCqZH1kui0XSX1JksV1WW8zETEdmA7Q1NTkX/1mZgXJ0w+DtI6h7Yx7\nF1Y4bQ6wo6QxJIniWODLOb9PwFXAwoi4LM85m8x1GGZmmSomjHQokM1JRqr9OXAkMLvSeRHRIuk0\n4A6gNzAjIp6UdGq6f5qkrUnGrBoMbJB0JjAO2Bn4CvC4pPnpJf93RNza0Rs0M7OukecNY6+I2FnS\nYxFxgaRLgdvyXDz9BX9rm23TSpZfISmqautPJL3Kq0Lu621mVlGesaTWpD//LunDJDPwbVNcSGZm\nVo/yvGHcImkI8EPgYZIWUj8rNKoacBWGmVm2rH4YfSNiXURclG66SdItQP+IeKs64VWHm9WamVWW\nVSS1VNLPJX0ubbVERKxttGRhZmb5ZCWMj5I0jf0e8JKkyyXtUZ2wzMys3pRNGBGxMiJ+mo4lNRFY\nAvyHpGcl/aBqEVaLKzHMzDLlaSVFRLxM0pHuJ8Bq4JQig6o6V2KYmVWUmTAk9Zd0lKSbgcUkQ5pP\nIZkfw8zMepCsVlK/AvYD7gOuA74cEW9XKzAzM6svWf0wbge+HhGrqxVMTbkOw8wsU9ZotddWMxAz\nM6tvuSq9G54rvc3MKnLCMDOzXComjLSV1KB0+XuSbpY0ofjQzMysnuR5w/jXiFgtaR+SVlOt/TEa\niuu8zcyy5UkY69OfhwDTI+L3QL/iQqo+V2GYmVWWJ2EslfRT4BjgVkmb5TzPzMwaSJ5f/EeTTLN6\nYES8CQwFzi00qlpwmZSZWaY8EyhtA/w+ItZK+gzJfNsN1UfDU7SamVWW5w3jJmC9pH8ApgOjgF8V\nGpWZmdWdPAljQ0S0AEcAV0TEuTTYnN6u9DYzqyxPwlgn6Tjgn4Bb0m19iwupRlyHYWaWKU/CmATs\nCfwgIp6TNAb4RbFhmZlZvamYMCJiAXAO8LikjwHNEXFJ4ZFVlcukzMwqqdhKKm0ZNRN4HhAwStKJ\nEfHHYkMzM7N6kqdZ7aXAARGxCEDSR4Drgd2KDMzMzOpLnjqMvq3JAiAinqYBK71d521mli3PG8Zc\nST8HfpmuHw/MLS4kMzOrR3kSxjeAbwKnp+v3A/9VWES1EPgVw8ysgsyEIak3MCMijgcuq05IZmZW\njzLrMCJiPbCdpIYazvx93NXbzKyiPEVSS4A/S5oF/K11Y0T4jcPMrAfJkzCeTT+9gEHFhmNmZvWq\nbMKQ1B8YFBEXtNn+IWBV0YFVU4Arvc3MKsiqw/gx8Ml2tu8N/Ecx4ZiZWb3KShi7RcTNbTdGxG+B\nTxUXUvXJld5mZhVlJYzNO3memZk1oKxf/MslTWy7UdIngBV5Li7pIEmLJC2WNKWd/WMlPShpraRz\nOnKumZlVV1YrqXOBGyVdA8xLtzWRTKR0bKULp53+pgL7A83AHEmz0uHSW71O0oP88E6ca2ZmVVT2\nDSMiZgMTSYY0/2r6EbB7RDyU49oTgcURsSQi3gFuAA5r8x3LI2IOsK6j53Y1N5IyM8uW2Q8jIpYD\n53Xy2iOAl0rWm4Hdq3Buh4XHkjIzq6jbV15LmixprqS5K1bkqloxM7NOKDJhLAVGlayPTLd16bkR\nMT0imiKiafjw4Z0KVJ6i1cysoiITxhxgR0lj0sELjwVmVeFcMzMrQNbQIP8N5f/0johDsy4cES2S\nTgPuAFqHSX9S0qnp/mmStiaZjGkwsEHSmcC4iFjV3rkdvLeOcRWGmVmmrErvH6U/jwC25t0Z944D\nXs1z8Yi4Fbi1zbZpJcuvkBQ35Tq3KElHb2cMM7MsZRNGRNwHIOnSiGgq2fXfkhprilYPDWJmVlGe\nOowtJG3fuiJpDLBFcSGZmVk9yjMfxlnAvZKWkJTbbAd8vdCozMys7lRMGBFxu6QdgbHppqciYm2x\nYZmZWb3JaiV1RJldO0iivaHPzcyscWW9YXwxY18ADZMw1gzZhi22GFzrMMzM6lpWK6lJ1Qyklpbu\nfDADt698nJlZT1axlZSkLSVd1jpek6RLJW1ZjeCqxa1qzcwqy9OsdgawGjg6/awCri4yKDMzqz95\nmtXuEBFfKlm/QNL8ogIyM7P6lOcNY42kfVpXJO0NrCkuJDMzq0d53jBOBa4tqbd4AzixuJBqw/Mn\nmZlly9Nx71FgF0mD0/VVhUdVZa70NjOrrGyRlKRrSpZPjIhVjZgszMwsn6w6jF1Kls8oOhAzM6tv\nWQnDBTVmZrZRVh3GSEk/JhmhtnV5o4g4vdDIzMysrmQljHNLlhtrwqQ2xoyBYcNqHYWZWX3LGktq\nZjUDqaV99611BGZm9S9Pxz0zMzMnDDMzyyerH8aojH1fKCYcMzOrV1lvGHdKGt12o6STgMuLCsjM\nzOpTVsI4G/hDOp83AJK+A5wFfLrowMzMrL5ktZK6VdJa4DZJhwOnABOBT0XEG9UK0MzM6kNmpXdE\n3A1MAu4Ftgc+62RhZtYzlX3DkLSaZHgQAZsBnwOWSxIQETG4OiGamVk9UDTQ2N6SVgAvdPL0DwKv\ndWE4tdQo99Io9wG+l3rUKPcBm3Yv20XE8DwHNlTC2BSS5kZEU63j6AqNci+Nch/ge6lHjXIfUL17\nccc9MzPLxQnDzMxyccJ41/RaB9CFGuVeGuU+wPdSjxrlPqBK9+I6DDMzy8VvGGZmlkuPTxiSDpK0\nSNJiSVNqHU9HSXpe0uOS5kuam24bKulOSc+kPz9Q6zjbI2mGpOWSnijZVjZ2Sd9Jn9MiSQfWJur2\nlbmX8yUtTZ/NfEmfL9lXl/ciaZSkeyQtkPSkpDPS7d3uuWTcS7d6LpL6S5ot6dH0Pi5It1f/mURE\nj/0AvYFnSXqx9wMeBcbVOq4O3sPzwAfbbPu/wJR0eQpwSa3jLBP7p4AJwBOVYgfGpc9nM2BM+tx6\n1/oeKtzL+cA57Rxbt/cCbANMSJcHAU+n8Xa755JxL93quZB0nh6YLvcFHgL2qMUz6elvGBOBxRGx\nJCLeAW4ADqtxTF3hMKB1xsSZwOE1jKWsiPgj8HqbzeViPwy4ISLWRsRzwGKS51cXytxLOXV7LxGx\nLCIeTpdXAwuBEXTD55JxL+XU5b1E4q/pat/0E9TgmfT0hDECeKlkvZns/6HqUQB3SZonaXK6bauI\nWJYuvwJsVZvQOqVc7N31Wf2zpMfSIqvWIoNucS/p9Aa7kvxF262fS5t7gW72XCT1ljQfWA7cGRE1\neSY9PWE0gn0iYjxwMPBNSZ8q3RnJO2q3bArXnWNP/YSkuHM8sAy4tLbh5CdpIHATcGZErCrd192e\nSzv30u2eS0SsT/+djwQmSvpYm/1VeSY9PWEsBUpnFhyZbus2ImJp+nM58FuSV89XJW0DkP5cXrsI\nO6xc7N3uWUXEq+k/9A3Az3i3WKCu70VSX5JfsNdFxM3p5m75XNq7l+76XAAi4k3gHuAgavBMenrC\nmAPsKGmMpH7AscCsGseUm6QtJA1qXQYOAJ4guYcT08NOBH5Xmwg7pVzss4BjJW0maQywIzC7BvHl\n1vqPOfWPJM8G6vhe0tGorwIWRsRlJbu63XMpdy/d7blIGi5pSLo8ANgfeIpaPJNatwCo9Qf4PEnr\niWeB79Y6ng7Gvj1Ja4hHgSdb4weGAXcDzwB3AUNrHWuZ+K8nKRJYR1LOenJW7MB30+e0CDi41vHn\nuJdfAI8Dj6X/iLep93sB9iEp2ngMmJ9+Pt8dn0vGvXSr5wLsDDySxvsE8H/S7VV/Ju7pbWZmufT0\nIikzM8vJCcPMzHJxwjAzs1ycMMzMLBcnDDMzy8UJw3oMSYdLCkljS7aNLh1htsx5FY+pcG6zpF5t\nts+XtHvGeV+VdGVnvtOsKE4Y1pMcB/wp/VkVEfE88CLwydZtacIaFMl4QGbdhhOG9QjpeEL7kHSo\nO7bMMV+V9DtJ96ZzDJxXsru3pJ+l8xH8Ie1xi6SvSZqTzlVwk6TN27n09W2+81iSkZGR9EVJD0l6\nRNJdkt43UKSkayQdWbL+15Llc9Pvf6xknoQtJP0+jekJScfk/e9klsUJw3qKw4DbI+JpYKWk3coc\nNxH4Eknv2qMkNaXbdwSmRsROwJvpMQA3R8QnImIXkuGzT27nmjcCh0vqk64fQ5JEIHnj2SMidiVJ\nIt/Oe0OSDkjjmkgykN5u6eCTBwEvR8QuEfEx4Pa81zTL4oRhPcVxpH/Vpz/LFUvdGRErI2INcDPJ\nWwnAcxExP12eB4xOlz8m6X5JjwPHAzu1vWBEvEoypMPnJI0HWiKitU5kJHBHev657Z2f4YD08wjw\nMDCWJIHcVYJzAAABhUlEQVQ8Duwv6RJJn4yItzpwTbOy+lQ+xKx7kzQU+CzwcUlBMtNiSDq3ncPb\njpXTur62ZNt6YEC6fA1weEQ8KumrwGfKhNFaLPUq775dAFwBXBYRsyR9hmQ2uLZaSP+4SyvP+7Xe\nGvDvEfHTtidImkAybtL3Jd0dEReWicssN79hWE9wJPCLiNguIkZHxCjgOUoqokvsn86VPIBkBrM/\nV7j2IGBZOoz28RnH3UzyC/wY3n3TAdiSd4eePrHtSanngdYitENJZlwDuAM4Ka2fQdIISR+S9GHg\n7xHxS+CHJFPHmm0yJwzrCY4jmSuk1E20Xyw1O933GHBTRMytcO1/JZnF7c8kQ063K5J5DB4EXo2I\nJSW7zgd+LWke8FqZ038GfFrSo8CewN/Sa/4B+BXwYFqk9RuSBPZxYHY6Q9t5wPcr3INZLh6t1iyV\nFik1RcRptY7FrB75DcPMzHLxG4aZmeXiNwwzM8vFCcPMzHJxwjAzs1ycMMzMLBcnDDMzy8UJw8zM\ncvn/enxWBakP9ZkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a86beb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum MAE Value is 0.09553\n",
      "The best parameter alpha is 0.00010\n"
     ]
    }
   ],
   "source": [
    "#Create parameter : alpha value list\n",
    "alpha= np.linspace(0.0001,300,1000)\n",
    "\n",
    "#Set up the function entries\n",
    "x = x_train #80% of the original dataset\n",
    "y = y_train\n",
    "clf = lasreg #Lasso Regression model\n",
    "param_values = alpha  #Parameter: alpha values\n",
    "param_name = 'alpha' #Parameter name\n",
    "K = 5 #Number of KFold\n",
    "\n",
    "#Call the calc_params function for Ridge Regression\n",
    "a,b,c,d=calc_params(x, y, clf, param_values, param_name,K)\n",
    "print('The minimum MAE Value is %0.5f'% (c))\n",
    "print('The best parameter alpha is %0.5f'%(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is  0.0959257086164\n",
      "RMSE is  0.130668754283\n"
     ]
    }
   ],
   "source": [
    "#Use the Best parameter: alpha run for the 20% Test Set\n",
    "lasreg.set_params(alpha=0.0001) #Use the best parameter: alpha for predicting the test set\n",
    "lasreg.fit(x_train,y_train)\n",
    "test_predict = lasreg.predict(x_test)\n",
    "MAE=mean_absolute_error(y_test,test_predict) #Mean Absolute Error of test set\n",
    "err= test_predict - y_test\n",
    "rmse=np.sqrt(np.dot(err,err)/len(y_test))\n",
    "print('MAE is ', MAE)\n",
    "print('RMSE is ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "According to the above Ridge and Lasso Regression analysis. \n",
    "\n",
    "The Mean Absolute Error (MAE) of Ridge Regression for the training set (80% of original data) is 0.09511 with Alpha 4.0017. By observing the plot (MAE vs Alpha), there is NO obvious underfitting or overfitting occurs. Also, the graph shows that the higher value of alpha gives the higher value of errors. For the Testing set analysis, the MAE is 0.096027 with Alpha. The MAE value between Training and testing set is very close, so that it could prove that there is NO overfitting occuring in the case.\n",
    "\n",
    "The Mean Absolute Error (MAE) of Lasso Regression for trainging set (20% of original data) is 0.09553 with Alpha 0.0001. By observing the plot (MAE vs Alpha), the MAE value are very close between train and test set of Training data. However, the change of value of alpha does NOT give any improvement on the MAE. For the Testing set analysis, The MAE is 0.09592 with Alpha 0.0001. \n",
    "\n",
    "In this particular case, we could see that the alpha value of Lasso which is 0.0001 is very small. The feature of Lasso model adds penalty equivalent to absolute value of the magnitude of coefficients. So that the small value alpha 0.0001 shows the Lasso model does not make a lot of adjustment / penalty to the original model. Another feeature of Lasso is that it could help to perform a feature selection. In this case, the feature selection feature does not quite function well when the small alpha 0.0001. On the contrary, the Ridge Regression with Alpha 4.0017 shows the optimal result. Also, Ridge helps to improve the over-fitting problem in this case because  the higher of Alpha tends to have lower MAE, OR might even turns into under-fitting. To sum up, Ridge Regression would be the best model for this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1e) Next, perform regression using Stochastic Gradient Descent for regression. For this part, you should use the SGDRegessor module from sklearn.linear_model. Again, start with creating randomized 80%-20% train-test split. SGDRegessor requires that features be standardized (with 0 mean and scaled by standard deviation). Prior to fiting the model, perform the scaling using StandardScaler from sklearn.preprocessing. For this problem, perform a grid search (using GridSearchCV from sklearn.grid_search) Your grid search should compare combinations of two penalty parameters ('l2', 'l1') and different values of alpha (alpha could vary from 0.0001 which is the default to relatively large values). Using the best parameters, apply the model to the set-aside test data. Finally, perform model selection (similar to part d, above) to find the best \"l1_ratio\" parameter using SGDRegressor with  the \"elasticnet\" penalty parameter. [Note: \"l1_ratio\" is The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1;  l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1 penalty; defaults to 0.15.] Using the best mixing ratio, apply the Elastic Net model to the set-aside test data. Provide a summary of your findings from the above experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x1)\n",
    "x_s = scaler.transform(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x1))\n",
    "print(type(y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_s,y1, test_size=0.2, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#Preset SGDRegressor parameters\n",
    "sgdreg=SGDRegressor()\n",
    "sgdpara={'penalty':['l2','l1']\n",
    "         ,'alpha': np.linspace(0.0001,100,500)}\n",
    "        #,'l1_ratio': np.linspace(0.1,1,200)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'average': False,\n",
       " 'epsilon': 0.1,\n",
       " 'eta0': 0.01,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'loss': 'squared_loss',\n",
       " 'n_iter': 5,\n",
       " 'penalty': 'l2',\n",
       " 'power_t': 0.25,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the parameter for the SDGRegression function\n",
    "sgdreg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Search out the best parameter\n",
    "sgd_gs=GridSearchCV(sgdreg,sgdpara,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l2', 'l1'], 'alpha': array([  1.00000e-04,   2.00501e-01, ...,   9.97996e+01,   1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_gs.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) The optimal parameter: alpha is 0.400901202405\n",
      "2 ) The optimal parameter: penalty is l2\n"
     ]
    }
   ],
   "source": [
    "for i,(para,value) in enumerate (sgd_gs.best_params_.items()):\n",
    "    print(i+1,') The optimal parameter:',para,'is',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse is 0.134 with penaly: l2 and alpha:0.0001\n"
     ]
    }
   ],
   "source": [
    "sgdreg=SGDRegressor(penalty='l2', alpha=0.4) #plug in the optimal parameters from the Grid Search\n",
    "sgdreg.fit(x_train,y_train) #Use the 80% train data to create model\n",
    "predict_y = sgdreg.predict(x_test) #Predict the 20% test data\n",
    "err = predict_y-y_test\n",
    "rmse = np.sqrt(np.dot(err,err)/len(y_test))\n",
    "MAE = mean_absolute_error()\n",
    "print('rmse is %0.3f'% (rmse), 'with penaly: l2 and alpha:0.0001' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to find the best parameters\n",
    "def calc_params(x, y, clf, param_values, param_name, param_name2,param_values2,K):\n",
    "    train_scores = np.zeros(len(param_values))\n",
    "    test_scores = np.zeros(len(param_values))\n",
    "    \n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "\n",
    "\n",
    "    for i , param_value in enumerate(param_values):\n",
    "        clf.set_params(**{param_name:param_value\n",
    "                          ,param_name2:param_value2})\n",
    "        cv = KFold(len(x), K, shuffle=True, random_state=0)\n",
    "\n",
    "        mae_err_train=np.zeros(K)\n",
    "        mae_err_test=np.zeros(K)\n",
    "\n",
    "\n",
    "        for j , (train, test) in enumerate(cv):\n",
    "            clf.fit([x[k] for k in train],y[train])\n",
    "            train_predict=clf.predict(x[train])\n",
    "            test_predict=clf.predict(x[test])\n",
    "            #Find the Mean Absolute Error for the Regression on Train and Test result\n",
    "            mae_err_train[j]=mean_absolute_error(y[train],train_predict)\n",
    "            mae_err_test[j]=mean_absolute_error(y[test],test_predict)\n",
    "        \n",
    "        #Take the average of the Mean Absolute Error for every parameter value\n",
    "        train_scores[i]=np.mean(mae_err_train)\n",
    "        test_scores[i]=np.mean(mae_err_test)\n",
    "    \n",
    "    #Find the Best score for test result\n",
    "    min_test_scores = test_scores[np.where(test_scores==test_scores.min())][0]\n",
    "    #min_test_scores = min(test_scores)\n",
    "    #Find the best parameter value\n",
    "    best_param = param_values[np.where(test_scores==test_scores.min())][0]\n",
    "    #best_param = param_values[argmin(test_scores)]\n",
    "    \n",
    "    \n",
    "    plt.plot(param_values,train_scores, color='blue', label='Train',alpha=0.4)\n",
    "    plt.plot(param_values,test_scores, color='red', label='Test',alpha=0.4)\n",
    "    plt.xlabel('Alpha Values')\n",
    "    plt.ylabel('K Fold Cross Validation MAE Error Accuracy')\n",
    "    plt.legend(loc=7)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_scores , test_scores, min_test_scores, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIFEURVFaQgWJoiiK0hL/D1Anovi7DsOZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111e38518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum MAE Value is 0.09552\n",
      "The best parameter l1_ratio is 0.91523\n"
     ]
    }
   ],
   "source": [
    "#Create parameter : alpha value list\n",
    "l1_ratio= np.linspace(0.1,1,500)\n",
    "\n",
    "#Set up the function entries\n",
    "x = x_s #80% of the original dataset\n",
    "y = y1\n",
    "clf = sgdreg #SDG Regression model\n",
    "param_values = l1_ratio  #Parameter: l1_rato values\n",
    "param_name = 'l1_ratio' #Parameter name\n",
    "param_values2 = 'elasticnet'  #Parameter: penalty values\n",
    "param_name2 = 'penalty' #Parameter name\n",
    "K = 5 #Number of KFold\n",
    "\n",
    "#Call the calc_params function for Ridge Regression\n",
    "a,b,c,d=calc_params(x, y, clf, param_values, param_name, param_name2,param_values2,K)\n",
    "print('The minimum MAE Value is %0.5f'% (c))\n",
    "print('The best parameter l1_ratio is %0.5f'%(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse is 0.12580 with penalty=elasticnet, alpha=0.00010, l1_ratio=0.91523, iteration=300 \n",
      "MAE is 0.09275 with penalty=elasticnet, alpha=0.00010, l1_ratio=0.91523, iteration=300 \n"
     ]
    }
   ],
   "source": [
    "penalty_val='elasticnet'\n",
    "alpha_val=0.0001\n",
    "l1_ratio_val=0.91523\n",
    "n_iter_val=300\n",
    "\n",
    "sgdreg=SGDRegressor(penalty=penalty_val, alpha=alpha_val,l1_ratio=l1_ratio_val, n_iter=n_iter_val)\n",
    "sgdreg.fit(x_s,y1)\n",
    "predict_y = sgdreg.predict(x_test)\n",
    "err=predict_y - y_test\n",
    "rmse=np.sqrt(np.dot(err,err)/len(y_test))\n",
    "MAE=mean_absolute_error(predict_y,y_test)\n",
    "print('rmse is %0.5f with penalty=%s, alpha=%0.5f, l1_ratio=%0.5f, iteration=%d '\n",
    "      %(rmse,penalty_val,alpha_val,l1_ratio_val,n_iter_val))\n",
    "\n",
    "print('MAE is %0.5f with penalty=%s, alpha=%0.5f, l1_ratio=%0.5f, iteration=%d '\n",
    "      %(MAE,penalty_val,alpha_val,l1_ratio_val,n_iter_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on the above experiment results, we could observe that using the Grid Search function really help to search out the best parameter for the dataset. The MAE and RMSE result on the test dataset are very close to the previous analysis (Standard Linear Regression, Ridge Regression, Lasso Regression). However, the Grid Search function definitely increase the efficiency of choosing the best parameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####2)Automatic Document Clustering [Dataset: newsgroups5.zip]\n",
    "####For this problem you will use a different subset of the 20 Newsgroup data set that you used in Assignment 2  (see the description of the full dataset). The subset for this assignment includes 2,500 documents (newsgroup posts), each belonging to one of 5 categories windows (0), crypt (1), christian (2), hockey (3), forsale (4). The documents are represented by 9328 terms (stems). The dictionary (vocabulary) for the data set is given in the file \"terms.txt\" and the full term-by-document matrix is given in \"matrix.txt\" (comma separated values). The actual category labels for the documents are provided in the file \"classes.txt\". Your goal in this assignment is to perform clustering on the documents and compare the clusters to the actual categories.\n",
    "\n",
    "####Your tasks in this problem are the following [Note: for the clustering part of this assignment you should use the kMeans module form Ch. 10 of MLA (use the version provided here as it includes some corrections to the book version). You may also use Pandas and other modules from scikit-learn that you may need for preprocessing or evaluation.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####2a) Create your own distance function that, instead of using Euclidean distance, uses Cosine similarity. This is the distance function you will use to pass to the kMeans function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "# x = the instance to be classified\n",
    "# D = training data matrix\n",
    "\n",
    "def disCosine(x,D):\n",
    "    dot_pro = np.dot(D,x)\n",
    "    x_norm = np.linalg.norm(x)\n",
    "    D_norm = np.array([np.linalg.norm(D[i]) for i in range(len(D))])\n",
    "    sims=dot_pro / (x_norm * D_norm)\n",
    "    dists=1-sims\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####2b) Load the data set [Note: the data matrix provided has terms as rows and documents as columns. Since you will be clustering documents, you'll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the \".T\" operation to obtain the transpose.] Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./newsgroups5/matrix.txt',header=None,na_values='?')\n",
    "df_class = pd.read_csv('./newsgroups5/classes.txt',sep=' ')\n",
    "terms = pd.read_csv('./newsgroups5/terms.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.isnull().sum(0)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is NO missing values based on the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.T\n",
    "df_class=df_class.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=np.array(df)\n",
    "df_class=np.array(df_class)\n",
    "terms=np.array(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 9328)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9328, 1)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  2500 documents\n",
      "There are  9328 terms\n"
     ]
    }
   ],
   "source": [
    "N_doc_train=df.shape[0]\n",
    "N_terms=terms.shape[0]\n",
    "print('There are ',N_doc_train,'documents')\n",
    "print('There are ',N_terms,'terms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['aa'],\n",
       "       ['aargh'],\n",
       "       ['aaron'],\n",
       "       ['aaronc'],\n",
       "       ['ab']], dtype=object)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train,x_test,y_train, y_test = train_test_split(df,df_class,test_size=0.2,random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9328)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9328)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####2c) As in the case of Assignment 2, transform the term-frequencies to tfxidf values. Be sure to maintain DF values for each of the terms in the dictionary. [Note: if you run into problems due to limited computational resources, you may prune the data by removing all terms with low DF values, e.g., terms that appear in less than 10 documents. Be sure to maintain the correspondence between the dictionary terms and the matrix rows.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9328)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a New Dataframe with the same size as the train data set\n",
    "N_matrix_train=np.ones(x_train.shape,dtype=float) * N_doc_train\n",
    "N_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9328)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total Number of Document contains every Vocabulary \n",
    "df_train=np.array([(x_train!=0).sum(0)])\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6441"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(df_train<10)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([   0,    1,    3, ..., 9325, 9326, 9327]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df_train_less10=np.where(df_train<10)\n",
    "index_df_train_less10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2887,)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing the df which appear in less than 10 in documents\n",
    "df_train=np.delete(df_train, index_df_train_less10[1])\n",
    "df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2887)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing the columns corresponding to the df terms above\n",
    "N_matrix_train=np.delete(N_matrix_train, index_df_train_less10[1],1)\n",
    "N_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2887)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=np.delete(x_train,index_df_train_less10[1],1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2887)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=np.delete(x_test,index_df_train_less10[1],1)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2887)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms=terms.T\n",
    "terms=np.delete(terms,index_df_train_less10[1],1)\n",
    "terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find any ZERO appear in the df_train matrix\n",
    "index_df_train_zero=np.where(df_train==0)\n",
    "df_train[np.where(df_train==0)]\n",
    "#print('index in df_train dataset is Zero: ',index_df_train_zero[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf=log(N/nk)\n",
    "idf_train=np.log2(np.divide(N_matrix_train,df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-96899586e5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Now we will set the term column values to ZERO to columns of df_train which are equal to ZERO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcol_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_df_train_zero\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Display the undividable columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0midf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#Now we will set the term column values to ZERO to columns of df_train which are equal to ZERO\n",
    "for col_index in index_df_train_zero[1]:\n",
    "    print(idf_train[:,col_index]) #Display the undividable columns\n",
    "    idf_train[:,col_index]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-cbae40433c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Double check if the above 'set to zero' works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcol_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_df_train_zero\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#Double check if the above 'set to zero' works\n",
    "for col_index in index_df_train_zero[1]:\n",
    "    print(idf_train[:,col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2887)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tf_idf for train and test data\n",
    "tf_idf_train=x_train * idf_train\n",
    "tf_idf_test=x_test * idf_train[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2887)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2887)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "2d) Perform Kmeans clustering on the training data. Write a function to display the top N terms in each cluster along with the cluster DF values for each term and the size of the cluster. Cluster DF value for a term t in a cluster C is the percentage of docs in cluster C in which term t appears. Sort the terms in decreasing order of the DF percentage. Here is an example of how this output might look like (here the top 10 terms for 3 of the 5 clusters are displayed in decreasing order of cluster DF values, but the mean frequnecy from the cluster centroid is also shown). [Extra Credit: use your favorite third party tool, ideally with a Python based API, to create a word cloud for each cluster based on the in-cluster DF values.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'kMeans' from '/Users/KevQuant/Desktop/Depaul/csc478/Assignment/assign3/kMeans.py'>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(kMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, clusters = kMeans.kMeans(mat(tf_idf_train), 5, distMeas=disCosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2887,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaron', 'ab', 'abc', ..., 'young', 'zero', 'zone'], dtype=object)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 10,  1, ..., 25,  7,  1])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-403-af718641a134>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-403-af718641a134>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sorted_terms,percent_term = [x,y for _,(x,y )in sorted(zip(DF_in_cluster,terms[0],percent_DF_in_cluster),reverse=True)]\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sorted_terms,percent_term = [x,y for _,(x,y )in sorted(zip(DF_in_cluster,terms[0],percent_DF_in_cluster),reverse=True)]\n",
    "sorted_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster : 0\n",
      "No of Documents in Cluster 1135\n",
      "words\tDF\tPercent\n",
      "subject\t1135\t1.00000\n",
      "write\t459\t0.40441\n",
      "on\t427\t0.37621\n",
      "know\t343\t0.30220\n",
      "articl\t330\t0.29075\n",
      "Cluster : 1\n",
      "No of Documents in Cluster 8\n",
      "words\tDF\tPercent\n",
      "ax\t8\t1.00000\n",
      "subject\t8\t1.00000\n",
      "mw\t7\t0.87500\n",
      "mq\t7\t0.87500\n",
      "max\t7\t0.87500\n",
      "Cluster : 2\n",
      "No of Documents in Cluster 426\n",
      "words\tDF\tPercent\n",
      "subject\t426\t1.00000\n",
      "write\t264\t0.61972\n",
      "articl\t204\t0.47887\n",
      "on\t193\t0.45305\n",
      "kei\t192\t0.45070\n",
      "Cluster : 3\n",
      "No of Documents in Cluster 398\n",
      "words\tDF\tPercent\n",
      "subject\t398\t1.00000\n",
      "game\t217\t0.54523\n",
      "write\t211\t0.53015\n",
      "team\t177\t0.44472\n",
      "go\t171\t0.42965\n",
      "Cluster : 4\n",
      "No of Documents in Cluster 33\n",
      "words\tDF\tPercent\n",
      "subject\t33\t1.00000\n",
      "sale\t14\t0.42424\n",
      "van\t13\t0.39394\n",
      "bo\t13\t0.39394\n",
      "det\t11\t0.33333\n"
     ]
    }
   ],
   "source": [
    "No_clusters=5 #\n",
    "top_No_terms=5\n",
    "for i in range(0,No_clusters):\n",
    "    print('Cluster :', i)\n",
    "    No_doc_cluster=sum(clusters[:,0]==i) #Number of documents in each cluster\n",
    "    print('No of Documents in Cluster',No_doc_cluster)\n",
    "    Doc_in_cluster= tf_idf_train[clusters[:,0]==i] #Sort out the document classified in each cluster\n",
    "    DF_in_cluster = (Doc_in_cluster!=0).sum(0) #Sum up the DF in each cluster\n",
    "    percent_DF_in_cluster= DF_in_cluster / No_doc_cluster\n",
    "    sorted_index=argsort(DF_in_cluster)[::-1]\n",
    "   \n",
    "    print('words\\tDF\\tPercent')\n",
    "    for k in range(0,top_No_terms):\n",
    "        print('%s\\t%d\\t%0.5f' %(terms[0][sorted_index][0:10][k]\n",
    "                         ,DF_in_cluster[sorted_index][0:10][k]\n",
    "                         ,percent_DF_in_cluster[sorted_index][0:10][k]))\n",
    "    #print(DF_in_cluster[sorted_index][0:10])\n",
    "    #print((percent_DF_in_cluster[sorted_index]*100)[0:10])\n",
    "    #print(terms[0][sorted_index][0:10])\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1135"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_cluster.shape\n",
    "sorted_term=[x for _,x in sorted(zip(tf_idf_cluster,terms))]\n",
    "sorted_term=np.array(sorted_term)\n",
    "for i in range(0,20):\n",
    "    print(sorted_term[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2887,)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_cluster= tf_idf_train[clusters[:,0]==1]\n",
    "tf_idf_cluster=(tf_idf_cluster!=0).sum(1)\n",
    "DF_term_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'd', 'h', 'b', 'c', 'e', 'i', 'f', 'g'], \n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"]))\n",
    "XX=np.array([\"aa\", \"bd\", \"cw\", \"dq\", \"ee\", \"fr\", \"gq\", \"he\", \"io\"])\n",
    "Y = np.array([ 0,   1,   1,    0,   1,   2,   2,   0,   1])\n",
    "indx=np.argsort(Y)\n",
    "X[indx]\n",
    "\n",
    "\n",
    "#Z = [x for _,x[0] in sorted(zip(Y,X,XX),reverse=True)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
